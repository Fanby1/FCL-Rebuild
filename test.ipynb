{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试spliter功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataloaders import ImageNetR_Spliter\n",
    "\n",
    "# spliter = ImageNetR_Spliter(client_num=5, task_num=5, private_class_num=40, input_size=224, path='C:/Users/Admin/datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client_subset,client_mask = spliter.random_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del client_subset,client_mask,spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:01<00:00, 39286.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[52, 15, 44, 83, 73, 33, 10, 78], [65, 42, 22, 38, 98, 25, 32, 57], [59, 66, 13, 40, 61, 77, 5, 63], [43, 94, 90, 46, 55, 6, 26, 79], [39, 7, 34, 53, 27, 16, 29, 47]], [[15, 30, 46, 42, 22, 63, 66, 29], [47, 55, 72, 9, 17, 77, 60, 11], [83, 59, 1, 95, 87, 0, 20, 98], [53, 79, 78, 13, 34, 12, 2, 32], [14, 85, 94, 27, 65, 7, 33, 61]], [[61, 19, 27, 23, 31, 32, 99, 7], [69, 15, 53, 55, 78, 66, 13, 29], [46, 83, 28, 22, 51, 82, 33, 98], [54, 50, 34, 59, 77, 47, 79, 63], [88, 41, 42, 68, 84, 80, 65, 94]], [[74, 32, 33, 35, 15, 78, 97, 65], [98, 77, 92, 3, 46, 29, 18, 83], [27, 59, 13, 91, 7, 61, 93, 67], [70, 79, 47, 36, 66, 53, 55, 24], [63, 89, 42, 94, 34, 22, 4, 45]], [[63, 86, 94, 42, 33, 96, 15, 13], [81, 34, 78, 79, 27, 59, 61, 71], [77, 56, 98, 8, 53, 22, 37, 49], [29, 64, 21, 47, 48, 32, 58, 66], [65, 75, 7, 46, 55, 62, 83, 76]]]\n"
     ]
    }
   ],
   "source": [
    "from dataloaders import Cifar100_Spliter\n",
    "\n",
    "spliter = Cifar100_Spliter(client_num=5, task_num=5, private_class_num=15, input_size=224, path='C:/Users/Admin/datasets')\n",
    "client_subset,client_mask = spliter.random_split()\n",
    "\n",
    "print(client_mask)\n",
    "\n",
    "# del client_subset,client_mask,spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 21306.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[52, 15, 44, 83, 73, 33, 10, 78], [65, 42, 22, 38, 98, 25, 32, 57], [59, 66, 13, 40, 61, 77, 5, 63], [43, 94, 90, 46, 55, 6, 26, 79], [39, 7, 34, 53, 27, 16, 29, 47]], [[15, 30, 46, 42, 22, 63, 66, 29], [47, 55, 72, 9, 17, 77, 60, 11], [83, 59, 1, 95, 87, 0, 20, 98], [53, 79, 78, 13, 34, 12, 2, 32], [14, 85, 94, 27, 65, 7, 33, 61]], [[61, 19, 27, 23, 31, 32, 99, 7], [69, 15, 53, 55, 78, 66, 13, 29], [46, 83, 28, 22, 51, 82, 33, 98], [54, 50, 34, 59, 77, 47, 79, 63], [88, 41, 42, 68, 84, 80, 65, 94]], [[74, 32, 33, 35, 15, 78, 97, 65], [98, 77, 92, 3, 46, 29, 18, 83], [27, 59, 13, 91, 7, 61, 93, 67], [70, 79, 47, 36, 66, 53, 55, 24], [63, 89, 42, 94, 34, 22, 4, 45]], [[63, 86, 94, 42, 33, 96, 15, 13], [81, 34, 78, 79, 27, 59, 61, 71], [77, 56, 98, 8, 53, 22, 37, 49], [29, 64, 21, 47, 48, 32, 58, 66], [65, 75, 7, 46, 55, 62, 83, 76]]]\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "_, test = spliter.process_testdata(0)\n",
    "print(client_mask)\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\fcl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from trainer import Trainer\n",
    "from utils.options import get_args\n",
    "args = get_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== 1 =======================\n",
      "Optimizer is reset!\n",
      "LR: 0.001\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from utils.utils import Logger\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "# duplicate output stream to output file\n",
    "if not os.path.exists(args.log_dir): os.makedirs(args.log_dir)\n",
    "log_out = args.log_dir + '/output.log'\n",
    "sys.stdout = Logger(log_out)\n",
    "\n",
    "# save args\n",
    "with open(args.log_dir + '/args.yaml', 'w') as yaml_file:\n",
    "    yaml.dump(vars(args), yaml_file, default_flow_style=False)\n",
    "\n",
    "metric_keys = ['acc','time',]\n",
    "save_keys = ['global', 'pt', 'pt-local']\n",
    "global_only = ['time']\n",
    "avg_metrics = {}\n",
    "for mkey in metric_keys: \n",
    "    avg_metrics[mkey] = {}\n",
    "    for skey in save_keys: avg_metrics[mkey][skey] = []\n",
    "    \n",
    "# load results\n",
    "if args.overwrite:\n",
    "    start_r = 0\n",
    "else:\n",
    "    try:\n",
    "        for mkey in metric_keys: \n",
    "            for skey in save_keys:\n",
    "                if (not (mkey in global_only)) or (skey == 'global'):\n",
    "                    save_file = args.log_dir+'/results-'+mkey+'/'+skey+'.yaml'\n",
    "                    if os.path.exists(save_file):\n",
    "                        with open(save_file, 'r') as yaml_file:\n",
    "                            yaml_result = yaml.safe_load(yaml_file)\n",
    "                            avg_metrics[mkey][skey] = np.asarray(yaml_result['history'])\n",
    "        # next repeat needed\n",
    "        start_r = avg_metrics[metric_keys[0]][save_keys[0]].shape[-1]\n",
    "        # extend if more repeats left\n",
    "        if start_r < args.repeat:\n",
    "            max_task = avg_metrics['acc']['global'].shape[0]\n",
    "            for mkey in metric_keys: \n",
    "                avg_metrics[mkey]['global'] = np.append(avg_metrics[mkey]['global'], np.zeros((max_task,args.repeat-start_r)), axis=-1)\n",
    "                if (not (mkey in global_only)):\n",
    "                    avg_metrics[mkey]['pt'] = np.append(avg_metrics[mkey]['pt'], np.zeros((max_task,max_task,args.repeat-start_r)), axis=-1)\n",
    "                    avg_metrics[mkey]['pt-local'] = np.append(avg_metrics[mkey]['pt-local'], np.zeros((max_task,max_task,args.repeat-start_r)), axis=-1)\n",
    "    except:\n",
    "        start_r = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 16671.85it/s]\n",
      "c:\\Users\\Admin\\repo\\FCL-Rebuild\\learners\\default.py:220: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(filename + 'class.pth'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m \tclient_data_val\u001b[38;5;241m.\u001b[39mappend(temp_val)\n\u001b[0;32m     15\u001b[0m surro_data, test_data \u001b[38;5;241m=\u001b[39m spliter\u001b[38;5;241m.\u001b[39mprocess_testdata(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_data_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m---> 17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_data_val\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mavg_metrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\repo\\FCL-Rebuild\\trainer.py:169\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, avg_metrics)\u001b[0m\n\u001b[0;32m    167\u001b[0m model_save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_top_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/models/repeat-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/client-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/task-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_names[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_index]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_save_dir): os\u001b[38;5;241m.\u001b[39mmakedirs(model_save_dir)\n\u001b[1;32m--> 169\u001b[0m avg_train_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurr_task\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# save model\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearner\u001b[38;5;241m.\u001b[39msave_model(model_save_dir)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\repo\\FCL-Rebuild\\learners\\default.py:110\u001b[0m, in \u001b[0;36mNormalNN.learn_batch\u001b[1;34m(self, train_loader, train_dataset, model_save_dir, val_loader, task_index)\u001b[0m\n\u001b[0;32m    107\u001b[0m \ty \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# model update\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m loss, output\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# measure elapsed time\u001b[39;00m\n\u001b[0;32m    113\u001b[0m batch_time\u001b[38;5;241m.\u001b[39mupdate(batch_timer\u001b[38;5;241m.\u001b[39mtoc())  \n",
      "File \u001b[1;32mc:\\Users\\Admin\\repo\\FCL-Rebuild\\learners\\default.py:153\u001b[0m, in \u001b[0;36mNormalNN.update_model\u001b[1;34m(self, inputs, targets, target_scores, dw_force, kd_index)\u001b[0m\n\u001b[0;32m    151\u001b[0m dw_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdw_k[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(targets\u001b[38;5;241m.\u001b[39msize())\u001b[38;5;241m.\u001b[39mlong()]\n\u001b[0;32m    152\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(inputs)\n\u001b[1;32m--> 153\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdw_cls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    156\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\repo\\FCL-Rebuild\\learners\\default.py:146\u001b[0m, in \u001b[0;36mNormalNN.criterion\u001b[1;34m(self, logits, targets, data_weights)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcriterion\u001b[39m(\u001b[38;5;28mself\u001b[39m, logits, targets, data_weights):\n\u001b[1;32m--> 146\u001b[0m \tloss_supervised \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m loss_supervised\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "client_data_train = []\n",
    "client_data_val = []\n",
    "for subset in client_subset:\n",
    "\ttemp_train = []\n",
    "\ttemp_val =[]\n",
    "\tfor data in subset:\n",
    "\t\ttrain_dataset, val_dataset = random_split(data, [int(len(data) * 0.7), len(data) - int(len(data) * 0.7)])\n",
    "\t\ttemp_train.append(train_dataset)\n",
    "\t\ttemp_val.append(val_dataset)\n",
    "\tclient_data_train.append(temp_train)\n",
    "\tclient_data_val.append(temp_val)\n",
    " \n",
    "surro_data, test_data = spliter.process_testdata(5)\n",
    "Trainer(args, 42, metric_keys, save_keys, train_dataset=client_data_train[0], test_dataset=test_data, \n",
    "        validate_dataset=client_data_val[0], client_index=1, class_mask=client_mask[0]).train(avg_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "powder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
