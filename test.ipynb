{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试spliter功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "statistic dataset by label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 57565.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic dataset by label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 55128.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from dataloaders import Cifar100_Spliter as Spliter\n",
    "\n",
    "spliter = Spliter(client_num=5, attacker_num=5, task_num=5, private_class_num=15, input_size=224, path='C:/Users/Admin/datasets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public class: [3, 5, 6, 9, 22, 31, 32, 35, 38, 41, 46, 54, 55, 56, 61, 65, 67, 73, 79, 81, 85, 90, 93, 97, 98]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<?, ?it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 10.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[9, 35, 88, 61, 85, 25, 72, 73], [37, 96, 22, 56, 54, 69, 90, 41], [97, 45, 63, 52, 38, 79, 67, 81], [93, 5, 70, 6, 55, 3, 50, 31], [27, 65, 19, 53, 98, 46, 2, 32]], [[11, 6, 81, 41, 54, 68, 32, 56], [83, 46, 31, 93, 35, 85, 82, 55], [97, 9, 67, 22, 38, 12, 1, 26], [61, 73, 49, 3, 90, 71, 8, 5], [51, 98, 42, 65, 79, 29, 89, 66]], [[56, 75, 61, 81, 85, 80, 14, 77], [22, 38, 64, 35, 90, 54, 9, 6], [46, 98, 58, 95, 41, 93, 13, 3], [73, 34, 60, 32, 55, 39, 86, 5], [78, 67, 79, 76, 65, 97, 23, 31]], [[3, 79, 46, 41, 67, 90, 9, 6], [18, 93, 4, 16, 31, 35, 22, 97], [55, 56, 59, 57, 73, 5, 32, 85], [21, 40, 99, 44, 0, 54, 65, 38], [81, 10, 61, 98, 91, 17, 15, 92]], [[46, 85, 97, 84, 93, 28, 41, 32], [67, 79, 38, 35, 6, 3, 73, 24], [7, 36, 98, 74, 22, 48, 9, 30], [43, 94, 65, 56, 55, 81, 87, 62], [47, 33, 31, 61, 54, 20, 90, 5]], [[9, 35, 88, 61, 85, 25, 72, 73], [37, 96, 22, 56, 54, 69, 90, 41], [97, 45, 63, 52, 38, 79, 67, 81], [93, 5, 70, 6, 55, 3, 50, 31], [27, 65, 19, 53, 98, 46, 2, 32]], [[46, 85, 97, 84, 93, 28, 41, 32], [67, 79, 38, 35, 6, 3, 73, 24], [7, 36, 98, 74, 22, 48, 9, 30], [43, 94, 65, 56, 55, 81, 87, 62], [47, 33, 31, 61, 54, 20, 90, 5]], [[11, 6, 81, 41, 54, 68, 32, 56], [83, 46, 31, 93, 35, 85, 82, 55], [97, 9, 67, 22, 38, 12, 1, 26], [61, 73, 49, 3, 90, 71, 8, 5], [51, 98, 42, 65, 79, 29, 89, 66]], [[3, 79, 46, 41, 67, 90, 9, 6], [18, 93, 4, 16, 31, 35, 22, 97], [55, 56, 59, 57, 73, 5, 32, 85], [21, 40, 99, 44, 0, 54, 65, 38], [81, 10, 61, 98, 91, 17, 15, 92]], [[56, 75, 61, 81, 85, 80, 14, 77], [22, 38, 64, 35, 90, 54, 9, 6], [46, 98, 58, 95, 41, 93, 13, 3], [73, 34, 60, 32, 55, 39, 86, 5], [78, 67, 79, 76, 65, 97, 23, 31]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "client_subset,client_mask = spliter.random_split()\n",
    "\n",
    "print(client_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 Task 0 has 1890 samples with targets: {35, 72, 73, 9, 85, 88, 25, 61}\n",
      "Client 0 Task 1 has 2011 samples with targets: {96, 69, 37, 41, 54, 22, 56, 90}\n",
      "Client 0 Task 2 has 1975 samples with targets: {97, 67, 38, 45, 79, 81, 52, 63}\n",
      "Client 0 Task 3 has 1695 samples with targets: {3, 5, 70, 6, 50, 55, 93, 31}\n",
      "Client 0 Task 4 has 2612 samples with targets: {32, 65, 2, 98, 46, 19, 53, 27}\n",
      "Client 1 Task 0 has 1581 samples with targets: {32, 68, 6, 41, 11, 81, 54, 56}\n",
      "Client 1 Task 1 has 1460 samples with targets: {35, 46, 82, 83, 85, 55, 93, 31}\n",
      "Client 1 Task 2 has 2119 samples with targets: {1, 97, 67, 38, 9, 12, 22, 26}\n",
      "Client 1 Task 3 has 2009 samples with targets: {3, 5, 71, 8, 73, 49, 90, 61}\n",
      "Client 1 Task 4 has 2773 samples with targets: {65, 98, 66, 42, 79, 51, 89, 29}\n",
      "Client 2 Task 0 has 2460 samples with targets: {75, 77, 14, 80, 81, 85, 56, 61}\n",
      "Client 2 Task 1 has 1166 samples with targets: {64, 35, 6, 38, 9, 22, 54, 90}\n",
      "Client 2 Task 2 has 1938 samples with targets: {98, 3, 41, 13, 46, 58, 93, 95}\n",
      "Client 2 Task 3 has 2317 samples with targets: {32, 34, 5, 39, 73, 86, 55, 60}\n",
      "Client 2 Task 4 has 1929 samples with targets: {97, 65, 67, 76, 78, 79, 23, 31}\n",
      "Client 3 Task 0 has 842 samples with targets: {67, 3, 6, 41, 9, 46, 79, 90}\n",
      "Client 3 Task 1 has 1934 samples with targets: {97, 35, 4, 16, 18, 22, 93, 31}\n",
      "Client 3 Task 2 has 1672 samples with targets: {32, 5, 73, 85, 55, 56, 57, 59}\n",
      "Client 3 Task 3 has 2731 samples with targets: {0, 65, 99, 38, 40, 44, 21, 54}\n",
      "Client 3 Task 4 has 2890 samples with targets: {98, 10, 15, 17, 81, 91, 92, 61}\n",
      "Client 4 Task 0 has 1677 samples with targets: {32, 97, 41, 46, 84, 85, 28, 93}\n",
      "Client 4 Task 1 has 1083 samples with targets: {67, 3, 35, 38, 6, 73, 79, 24}\n",
      "Client 4 Task 2 has 2868 samples with targets: {98, 36, 7, 9, 74, 48, 22, 30}\n",
      "Client 4 Task 3 has 2362 samples with targets: {65, 43, 81, 55, 87, 56, 94, 62}\n",
      "Client 4 Task 4 has 1947 samples with targets: {33, 5, 47, 20, 54, 90, 61, 31}\n"
     ]
    }
   ],
   "source": [
    "for client_id in range(5):\n",
    "    for task_id in range(5):\n",
    "        subset = client_subset[client_id][task_id]\n",
    "        targets = set(subset.targets)\n",
    "        print(f\"Client {client_id} Task {task_id} has {len(subset)} samples with targets: {targets}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Class 0 has 50000 samples\n",
      "Class 1 has 50000 samples\n",
      "Class 2 has 50000 samples\n",
      "Class 3 has 50000 samples\n",
      "Class 4 has 50000 samples\n",
      "Class 5 has 50000 samples\n",
      "Class 6 has 50000 samples\n",
      "Class 7 has 50000 samples\n",
      "Class 8 has 50000 samples\n",
      "Class 9 has 50000 samples\n",
      "Class 10 has 50000 samples\n",
      "Class 11 has 50000 samples\n",
      "Class 12 has 50000 samples\n",
      "Class 13 has 50000 samples\n",
      "Class 14 has 50000 samples\n",
      "Class 15 has 50000 samples\n",
      "Class 16 has 50000 samples\n",
      "Class 17 has 50000 samples\n",
      "Class 18 has 50000 samples\n",
      "Class 19 has 50000 samples\n",
      "Class 20 has 50000 samples\n",
      "Class 21 has 50000 samples\n",
      "Class 22 has 50000 samples\n",
      "Class 23 has 50000 samples\n",
      "Class 24 has 50000 samples\n",
      "Class 25 has 50000 samples\n",
      "Class 26 has 50000 samples\n",
      "Class 27 has 50000 samples\n",
      "Class 28 has 50000 samples\n",
      "Class 29 has 50000 samples\n",
      "Class 30 has 50000 samples\n",
      "Class 31 has 50000 samples\n",
      "Class 32 has 50000 samples\n",
      "Class 33 has 50000 samples\n",
      "Class 34 has 50000 samples\n",
      "Class 35 has 50000 samples\n",
      "Class 36 has 50000 samples\n",
      "Class 37 has 50000 samples\n",
      "Class 38 has 50000 samples\n",
      "Class 39 has 50000 samples\n",
      "Class 40 has 50000 samples\n",
      "Class 41 has 50000 samples\n",
      "Class 42 has 50000 samples\n",
      "Class 43 has 50000 samples\n",
      "Class 44 has 50000 samples\n",
      "Class 45 has 50000 samples\n",
      "Class 46 has 50000 samples\n",
      "Class 47 has 50000 samples\n",
      "Class 48 has 50000 samples\n",
      "Class 49 has 50000 samples\n",
      "Class 50 has 50000 samples\n",
      "Class 51 has 50000 samples\n",
      "Class 52 has 50000 samples\n",
      "Class 53 has 50000 samples\n",
      "Class 54 has 50000 samples\n",
      "Class 55 has 50000 samples\n",
      "Class 56 has 50000 samples\n",
      "Class 57 has 50000 samples\n",
      "Class 58 has 50000 samples\n",
      "Class 59 has 50000 samples\n",
      "Class 60 has 50000 samples\n",
      "Class 61 has 50000 samples\n",
      "Class 62 has 50000 samples\n",
      "Class 63 has 50000 samples\n",
      "Class 64 has 50000 samples\n",
      "Class 65 has 50000 samples\n",
      "Class 66 has 50000 samples\n",
      "Class 67 has 50000 samples\n",
      "Class 68 has 50000 samples\n",
      "Class 69 has 50000 samples\n",
      "Class 70 has 50000 samples\n",
      "Class 71 has 50000 samples\n",
      "Class 72 has 50000 samples\n",
      "Class 73 has 50000 samples\n",
      "Class 74 has 50000 samples\n",
      "Class 75 has 50000 samples\n",
      "Class 76 has 50000 samples\n",
      "Class 77 has 50000 samples\n",
      "Class 78 has 50000 samples\n",
      "Class 79 has 50000 samples\n",
      "Class 80 has 50000 samples\n",
      "Class 81 has 50000 samples\n",
      "Class 82 has 50000 samples\n",
      "Class 83 has 50000 samples\n",
      "Class 84 has 50000 samples\n",
      "Class 85 has 50000 samples\n",
      "Class 86 has 50000 samples\n",
      "Class 87 has 50000 samples\n",
      "Class 88 has 50000 samples\n",
      "Class 89 has 50000 samples\n",
      "Class 90 has 50000 samples\n",
      "Class 91 has 50000 samples\n",
      "Class 92 has 50000 samples\n",
      "Class 93 has 50000 samples\n",
      "Class 94 has 50000 samples\n",
      "Class 95 has 50000 samples\n",
      "Class 96 has 50000 samples\n",
      "Class 97 has 50000 samples\n",
      "Class 98 has 50000 samples\n",
      "Class 99 has 50000 samples\n"
     ]
    }
   ],
   "source": [
    "# del client_subset,client_mask,spliter\n",
    "\n",
    "print(len(spliter.train_index_by_class_label))\n",
    "for i in range(100):\n",
    "    print(f\"Class {i} has {len(spliter.train_index_by_class_label[i])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:01<00:00, 39286.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[52, 15, 44, 83, 73, 33, 10, 78], [65, 42, 22, 38, 98, 25, 32, 57], [59, 66, 13, 40, 61, 77, 5, 63], [43, 94, 90, 46, 55, 6, 26, 79], [39, 7, 34, 53, 27, 16, 29, 47]], [[15, 30, 46, 42, 22, 63, 66, 29], [47, 55, 72, 9, 17, 77, 60, 11], [83, 59, 1, 95, 87, 0, 20, 98], [53, 79, 78, 13, 34, 12, 2, 32], [14, 85, 94, 27, 65, 7, 33, 61]], [[61, 19, 27, 23, 31, 32, 99, 7], [69, 15, 53, 55, 78, 66, 13, 29], [46, 83, 28, 22, 51, 82, 33, 98], [54, 50, 34, 59, 77, 47, 79, 63], [88, 41, 42, 68, 84, 80, 65, 94]], [[74, 32, 33, 35, 15, 78, 97, 65], [98, 77, 92, 3, 46, 29, 18, 83], [27, 59, 13, 91, 7, 61, 93, 67], [70, 79, 47, 36, 66, 53, 55, 24], [63, 89, 42, 94, 34, 22, 4, 45]], [[63, 86, 94, 42, 33, 96, 15, 13], [81, 34, 78, 79, 27, 59, 61, 71], [77, 56, 98, 8, 53, 22, 37, 49], [29, 64, 21, 47, 48, 32, 58, 66], [65, 75, 7, 46, 55, 62, 83, 76]]]\n"
     ]
    }
   ],
   "source": [
    "from dataloaders import Cifar100_Spliter\n",
    "\n",
    "spliter = Cifar100_Spliter(client_num=5, task_num=5, private_class_num=15, input_size=224, path='C:/Users/Admin/datasets')\n",
    "client_subset,client_mask = spliter.random_split()\n",
    "\n",
    "print(client_mask)\n",
    "\n",
    "# del client_subset,client_mask,spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 21306.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[52, 15, 44, 83, 73, 33, 10, 78], [65, 42, 22, 38, 98, 25, 32, 57], [59, 66, 13, 40, 61, 77, 5, 63], [43, 94, 90, 46, 55, 6, 26, 79], [39, 7, 34, 53, 27, 16, 29, 47]], [[15, 30, 46, 42, 22, 63, 66, 29], [47, 55, 72, 9, 17, 77, 60, 11], [83, 59, 1, 95, 87, 0, 20, 98], [53, 79, 78, 13, 34, 12, 2, 32], [14, 85, 94, 27, 65, 7, 33, 61]], [[61, 19, 27, 23, 31, 32, 99, 7], [69, 15, 53, 55, 78, 66, 13, 29], [46, 83, 28, 22, 51, 82, 33, 98], [54, 50, 34, 59, 77, 47, 79, 63], [88, 41, 42, 68, 84, 80, 65, 94]], [[74, 32, 33, 35, 15, 78, 97, 65], [98, 77, 92, 3, 46, 29, 18, 83], [27, 59, 13, 91, 7, 61, 93, 67], [70, 79, 47, 36, 66, 53, 55, 24], [63, 89, 42, 94, 34, 22, 4, 45]], [[63, 86, 94, 42, 33, 96, 15, 13], [81, 34, 78, 79, 27, 59, 61, 71], [77, 56, 98, 8, 53, 22, 37, 49], [29, 64, 21, 47, 48, 32, 58, 66], [65, 75, 7, 46, 55, 62, 83, 76]]]\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "_, test = spliter.process_testdata(0)\n",
    "print(client_mask)\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\fcl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from trainer import Trainer\n",
    "from utils.options import get_args\n",
    "args = get_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== 1 =======================\n",
      "Optimizer is reset!\n",
      "LR: 0.001\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from utils.utils import Logger\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "# duplicate output stream to output file\n",
    "if not os.path.exists(args.log_dir): os.makedirs(args.log_dir)\n",
    "log_out = args.log_dir + '/output.log'\n",
    "sys.stdout = Logger(log_out)\n",
    "\n",
    "# save args\n",
    "with open(args.log_dir + '/args.yaml', 'w') as yaml_file:\n",
    "    yaml.dump(vars(args), yaml_file, default_flow_style=False)\n",
    "\n",
    "metric_keys = ['acc','time',]\n",
    "save_keys = ['global', 'pt', 'pt-local']\n",
    "global_only = ['time']\n",
    "avg_metrics = {}\n",
    "for mkey in metric_keys: \n",
    "    avg_metrics[mkey] = {}\n",
    "    for skey in save_keys: avg_metrics[mkey][skey] = []\n",
    "    \n",
    "# load results\n",
    "if args.overwrite:\n",
    "    start_r = 0\n",
    "else:\n",
    "    try:\n",
    "        for mkey in metric_keys: \n",
    "            for skey in save_keys:\n",
    "                if (not (mkey in global_only)) or (skey == 'global'):\n",
    "                    save_file = args.log_dir+'/results-'+mkey+'/'+skey+'.yaml'\n",
    "                    if os.path.exists(save_file):\n",
    "                        with open(save_file, 'r') as yaml_file:\n",
    "                            yaml_result = yaml.safe_load(yaml_file)\n",
    "                            avg_metrics[mkey][skey] = np.asarray(yaml_result['history'])\n",
    "        # next repeat needed\n",
    "        start_r = avg_metrics[metric_keys[0]][save_keys[0]].shape[-1]\n",
    "        # extend if more repeats left\n",
    "        if start_r < args.repeat:\n",
    "            max_task = avg_metrics['acc']['global'].shape[0]\n",
    "            for mkey in metric_keys: \n",
    "                avg_metrics[mkey]['global'] = np.append(avg_metrics[mkey]['global'], np.zeros((max_task,args.repeat-start_r)), axis=-1)\n",
    "                if (not (mkey in global_only)):\n",
    "                    avg_metrics[mkey]['pt'] = np.append(avg_metrics[mkey]['pt'], np.zeros((max_task,max_task,args.repeat-start_r)), axis=-1)\n",
    "                    avg_metrics[mkey]['pt-local'] = np.append(avg_metrics[mkey]['pt-local'], np.zeros((max_task,max_task,args.repeat-start_r)), axis=-1)\n",
    "    except:\n",
    "        start_r = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 16671.85it/s]\n",
      "c:\\Users\\Admin\\repo\\FCL-Rebuild\\learners\\default.py:220: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(filename + 'class.pth'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m \tclient_data_val\u001b[38;5;241m.\u001b[39mappend(temp_val)\n\u001b[0;32m     15\u001b[0m surro_data, test_data \u001b[38;5;241m=\u001b[39m spliter\u001b[38;5;241m.\u001b[39mprocess_testdata(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_data_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m---> 17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_data_val\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mavg_metrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\repo\\FCL-Rebuild\\trainer.py:169\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, avg_metrics)\u001b[0m\n\u001b[0;32m    167\u001b[0m model_save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_top_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/models/repeat-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/client-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/task-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_names[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_index]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_save_dir): os\u001b[38;5;241m.\u001b[39mmakedirs(model_save_dir)\n\u001b[1;32m--> 169\u001b[0m avg_train_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurr_task\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# save model\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearner\u001b[38;5;241m.\u001b[39msave_model(model_save_dir)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\repo\\FCL-Rebuild\\learners\\default.py:110\u001b[0m, in \u001b[0;36mNormalNN.learn_batch\u001b[1;34m(self, train_loader, train_dataset, model_save_dir, val_loader, task_index)\u001b[0m\n\u001b[0;32m    107\u001b[0m \ty \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# model update\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m loss, output\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# measure elapsed time\u001b[39;00m\n\u001b[0;32m    113\u001b[0m batch_time\u001b[38;5;241m.\u001b[39mupdate(batch_timer\u001b[38;5;241m.\u001b[39mtoc())  \n",
      "File \u001b[1;32mc:\\Users\\Admin\\repo\\FCL-Rebuild\\learners\\default.py:153\u001b[0m, in \u001b[0;36mNormalNN.update_model\u001b[1;34m(self, inputs, targets, target_scores, dw_force, kd_index)\u001b[0m\n\u001b[0;32m    151\u001b[0m dw_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdw_k[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(targets\u001b[38;5;241m.\u001b[39msize())\u001b[38;5;241m.\u001b[39mlong()]\n\u001b[0;32m    152\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(inputs)\n\u001b[1;32m--> 153\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdw_cls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    156\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\repo\\FCL-Rebuild\\learners\\default.py:146\u001b[0m, in \u001b[0;36mNormalNN.criterion\u001b[1;34m(self, logits, targets, data_weights)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcriterion\u001b[39m(\u001b[38;5;28mself\u001b[39m, logits, targets, data_weights):\n\u001b[1;32m--> 146\u001b[0m \tloss_supervised \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m loss_supervised\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "client_data_train = []\n",
    "client_data_val = []\n",
    "for subset in client_subset:\n",
    "\ttemp_train = []\n",
    "\ttemp_val =[]\n",
    "\tfor data in subset:\n",
    "\t\ttrain_dataset, val_dataset = random_split(data, [int(len(data) * 0.7), len(data) - int(len(data) * 0.7)])\n",
    "\t\ttemp_train.append(train_dataset)\n",
    "\t\ttemp_val.append(val_dataset)\n",
    "\tclient_data_train.append(temp_train)\n",
    "\tclient_data_val.append(temp_val)\n",
    " \n",
    "surro_data, test_data = spliter.process_testdata(5)\n",
    "Trainer(args, 42, metric_keys, save_keys, train_dataset=client_data_train[0], test_dataset=test_data, \n",
    "        validate_dataset=client_data_val[0], client_index=1, class_mask=client_mask[0]).train(avg_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
